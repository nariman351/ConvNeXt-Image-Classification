{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a184fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "import ConfigSpace as CS\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "decb467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda')\n",
    "classes = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "model_type = 'FFNN_Flat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2eb792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(data_train, data_val):\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    plt.rcParams[\"figure.figsize\"] = (13,13)\n",
    "    plt.rcParams[\"legend.fontsize\"] = 12\n",
    "    plt.xlabel('Epoch',fontsize=16)\n",
    "    plt.ylabel('Average Epoch Loss')\n",
    "    plt.suptitle('Average Loss')\n",
    "    plt.plot(data_train)\n",
    "    plt.plot(data_val)\n",
    "    plt.legend(['Training', 'Validation'], loc='upper right', fancybox=True)\n",
    "    plt.savefig(\"training_loss.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "def model_eval(features, labels, model, dataset):\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        predicted_classes = []\n",
    "        actual_classes = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(features):\n",
    "            inputs = torch.tensor(data).to(device)\n",
    "            label = torch.tensor(labels[i]).to(device)\n",
    "            output = model(inputs)\n",
    "            prediction = output.argmax(dim=-1, keepdim=True)\n",
    "            predicted_classes.append(prediction.item())\n",
    "            actual_classes.append(label.item())\n",
    "\n",
    "#         for i, data in enumerate(features):\n",
    "#             inputs = torch.tensor(data).to(device)\n",
    "#             labels = torch.tensor(labels[i]).to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             predictions = outputs.argmax(dim=-1, keepdim=True)\n",
    "#             predicted_classes.append(predictions.extend(predictions).item())\n",
    "#             actual_classes.append(labels.item())\n",
    "\n",
    "    #since labels are read in based on ordering in the folder, \n",
    "    #this corrects the labels so they reflect the correct classes\n",
    "    #############################################################\n",
    "    if dataset == 'gen':\n",
    "        labels_dict = {\n",
    "            0:3,\n",
    "            1:5,\n",
    "            2:7,\n",
    "            3:8\n",
    "        }\n",
    "        for index, item in enumerate(actual_classes):\n",
    "            actual_classes[index] = labels_dict[item]\n",
    "    #############################################################\n",
    "\n",
    "    performance_report = classification_report(\n",
    "                        actual_classes, \n",
    "                        predicted_classes, \n",
    "                        labels=list(range(0,10)), \n",
    "                        target_names=classes, \n",
    "                        output_dict=True\n",
    "                        )\n",
    "\n",
    "    with open(f'performance_report_{dataset}.json', 'w') as f:\n",
    "        json.dump(performance_report, f, indent=0)\n",
    "\n",
    "    overall_accuracy = accuracy_score(actual_classes, predicted_classes)\n",
    "\n",
    "    comparison_list = [['Actual', 'Predicted']]\n",
    "\n",
    "    for i in range(0,len(actual_classes)):\n",
    "        comparison_list.append([actual_classes[i], predicted_classes[i]])\n",
    "    np.savetxt(f'class_pred_{dataset}.csv', comparison_list, delimiter=',', fmt='%s')\n",
    "\n",
    "    labeled_actual = []\n",
    "    labeled_predicted = []\n",
    "    for index, item in enumerate(actual_classes):\n",
    "        labeled_actual.append(classes[actual_classes[index]])\n",
    "        labeled_predicted.append(classes[predicted_classes[index]])\n",
    "\n",
    "    plt.clf()\n",
    "    c_matrix = confusion_matrix(labeled_actual, labeled_predicted)\n",
    "    c_df = pd.DataFrame(c_matrix, index=classes, columns=classes)\n",
    "    plt.figure(figsize=(13,13))\n",
    "    sns.heatmap(c_df, annot=True, fmt='g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.savefig(f'labeled_confusion_matrix_{dataset}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return overall_accuracy\n",
    "\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        self.hl = config['h_layers']\n",
    "        \n",
    "        self.hidden_neurons = config['h_neurons']\n",
    "        \n",
    "        self.input = torch.nn.Linear(n_inputs, self.hidden_neurons)\n",
    "        \n",
    "        if config['h_layers']>=1:\n",
    "            self.hidden_1 = torch.nn.Linear(self.hidden_neurons, self.hidden_neurons)\n",
    "            \n",
    "        if config['h_layers']==2:\n",
    "            self.hidden_2 = torch.nn.Linear(self.hidden_neurons, self.hidden_neurons)\n",
    "        \n",
    "        self.output = torch.nn.Linear(self.hidden_neurons, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if self.hl>=1:\n",
    "            x = self.hidden_1(x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        if self.hl==2:\n",
    "            x = self.hidden_2(x)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee85ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_FFNN(config, checkpoint_dir=None):\n",
    "    xtrain = torch.load('C:/Users/s_kal/Desktop/9039-ML/Final Project/Code/train_extracted_features.pt',map_location=device)\n",
    "    ytrain = torch.load('C:/Users/s_kal/Desktop/9039-ML/Final Project/Code/train_extracted_labels.pt',map_location=device)\n",
    "    xtest = torch.load('C:/Users/s_kal/Desktop/9039-ML/Final Project/Code/val_extracted_features.pt',map_location=device)\n",
    "    ytest = torch.load('C:/Users/s_kal/Desktop/9039-ML/Final Project/Code/val_extracted_labels.pt',map_location=device)\n",
    "    \n",
    "#     For initial tuning:\n",
    "    model = NeuralNet(config)\n",
    "    model.to(device)\n",
    "    \n",
    "#     For learning curve of best model:\n",
    "#     plt.clf()\n",
    "#     plot_learning_curves(xtrain, ytrain, xtest, ytest, model)\n",
    "#     plt.savefig('learning_curve.png')\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    loss_hist = []\n",
    "    loss_val_hist = []\n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "        loss_epoch = []\n",
    "        loss_val_epoch = []\n",
    "        for i, data in enumerate(xtrain):\n",
    "            inputs = torch.tensor(data).to(device)\n",
    "            labels = torch.tensor(ytrain[i]).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss_epoch.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_hist.append(np.mean(loss_epoch))\n",
    "        for i, data in enumerate(xtest):\n",
    "            inputs = torch.tensor(data).to(device)\n",
    "            labels = torch.tensor(ytest[i]).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss_val_epoch.append(loss.item())\n",
    "        loss_val_hist.append(np.mean(loss_val_epoch))\n",
    "    \n",
    "    loss_result = [['Training', 'Validation']]\n",
    "    for i in range(0,len(loss_hist)):\n",
    "        loss_result.append([loss_hist[i], loss_val_hist[i]])\n",
    "    np.savetxt(f'loss.csv', loss_result, delimiter=',', fmt='%s')\n",
    "    \n",
    "#     np.savetxt('training_loss.csv', loss_hist, delimiter=',')\n",
    "#     np.savetxt('training_val_loss.csv', loss_val_hist, delimiter=',')\n",
    "    plot_loss(loss_hist, loss_val_hist)\n",
    "    \n",
    "    with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "        path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "    \n",
    "    train_acc =  model_eval(xtrain, ytrain, model, 'train')\n",
    "    val_acc = model_eval(xtest, ytest, model, 'val')\n",
    "    \n",
    "    tune.report(\n",
    "        train_ACC=train_acc,\n",
    "        val_ACC=val_acc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45a71c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining scale for hyperparameter tuning\n",
    "xtrain = torch.load('C:/Users/s_kal/Desktop/9039-ML/Final Project/Code/train_extracted_features.pt',map_location=device)\n",
    "n_inputs = xtrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cb407e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=15):\n",
    "    config = {\n",
    "        'h_layers':tune.choice([0, 1, 2]),\n",
    "        'h_neurons':tune.choice([64, 128, 256]),\n",
    "        'lr':tune.choice([1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n",
    "    }\n",
    "            \n",
    "    algo=TuneBOHB(metric='train_ACC', \n",
    "                  mode='max'\n",
    "                 )\n",
    "    \n",
    "    bohb = HyperBandForBOHB(time_attr=\"training_iteration\",\n",
    "                            metric=\"train_ACC\",\n",
    "                            mode=\"max\",\n",
    "                            max_t=1\n",
    "                           )\n",
    "        \n",
    "    result = tune.run(\n",
    "        tune.with_parameters(train_FFNN),\n",
    "        resources_per_trial={\"cpu\": 0, \"gpu\": 1},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=bohb,\n",
    "        search_alg=algo,\n",
    "        progress_reporter=tune.JupyterNotebookReporter(overwrite=True, print_intermediate_tables=True),\n",
    "        fail_fast=True, \n",
    "        sync_config=tune.SyncConfig(\n",
    "        syncer=None  # Disable syncing\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    result.results_df.to_csv(f'results_df_{model_type}.csv')\n",
    "    return result\n",
    "# BOHB - https://arxiv.org/abs/1807.01774\n",
    "# https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-bohb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cbfcbd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-07-23 08:18:21 (running for 06:07:36.17)<br>Memory usage on this node: 9.9/15.7 GiB<br>Using HyperBand: num_stopped=0 total_brackets=15\n",
       "Round #0:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #1:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #2:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #3:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #4:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #5:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #6:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #7:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #8:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #9:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #10:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #11:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #12:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #13:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} \n",
       "Round #14:\n",
       "  Bracket(Max Size (n)=1, Milestone (r)=1, completed=100.0%): {TERMINATED: 1} <br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/4.61 GiB heap, 0.0/2.31 GiB objects<br>Result logdir: C:\\Users\\s_kal\\ray_results\\train_FFNN_2022-07-23_02-10-45<br>Number of trials: 15/15 (15 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  h_layers</th><th style=\"text-align: right;\">  h_neurons</th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_ACC</th><th style=\"text-align: right;\">  val_ACC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_FFNN_3090b61c</td><td>TERMINATED</td><td>127.0.0.1:25400</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2035.05 </td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">   0.999 </td></tr>\n",
       "<tr><td>train_FFNN_30ab1ceb</td><td>TERMINATED</td><td>127.0.0.1:16352</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2214.47 </td><td style=\"text-align: right;\">   0.999933</td><td style=\"text-align: right;\">   0.9986</td></tr>\n",
       "<tr><td>train_FFNN_f08e50c1</td><td>TERMINATED</td><td>127.0.0.1:24244</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1204.43 </td><td style=\"text-align: right;\">   0.999844</td><td style=\"text-align: right;\">   0.9994</td></tr>\n",
       "<tr><td>train_FFNN_1ad3d0cd</td><td>TERMINATED</td><td>127.0.0.1:17064</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1320.08 </td><td style=\"text-align: right;\">   0.999978</td><td style=\"text-align: right;\">   0.9992</td></tr>\n",
       "<tr><td>train_FFNN_eb7c899c</td><td>TERMINATED</td><td>127.0.0.1:14532</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1030.84 </td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">   0.9992</td></tr>\n",
       "<tr><td>train_FFNN_00eba969</td><td>TERMINATED</td><td>127.0.0.1:3920 </td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1380.65 </td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">   0.9992</td></tr>\n",
       "<tr><td>train_FFNN_69dda953</td><td>TERMINATED</td><td>127.0.0.1:17996</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1642.44 </td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">   0.999 </td></tr>\n",
       "<tr><td>train_FFNN_a35ed4f0</td><td>TERMINATED</td><td>127.0.0.1:7496 </td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1277.45 </td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">   0.9994</td></tr>\n",
       "<tr><td>train_FFNN_78fd4315</td><td>TERMINATED</td><td>127.0.0.1:3216 </td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1016.06 </td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">   0.9994</td></tr>\n",
       "<tr><td>train_FFNN_74be7528</td><td>TERMINATED</td><td>127.0.0.1:24136</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">         64</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         953.644</td><td style=\"text-align: right;\">   0.9996  </td><td style=\"text-align: right;\">   0.9984</td></tr>\n",
       "<tr><td>train_FFNN_d51fb85f</td><td>TERMINATED</td><td>127.0.0.1:1660 </td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">        256</td><td style=\"text-align: right;\">0.1   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2046.96 </td><td style=\"text-align: right;\">   0.100289</td><td style=\"text-align: right;\">   0.0974</td></tr>\n",
       "<tr><td>train_FFNN_105dd71b</td><td>TERMINATED</td><td>127.0.0.1:14592</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1679.09 </td><td style=\"text-align: right;\">   0.999889</td><td style=\"text-align: right;\">   0.999 </td></tr>\n",
       "<tr><td>train_FFNN_d71c41d6</td><td>TERMINATED</td><td>127.0.0.1:22872</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1663.09 </td><td style=\"text-align: right;\">   0.999978</td><td style=\"text-align: right;\">   0.9978</td></tr>\n",
       "<tr><td>train_FFNN_c29779bc</td><td>TERMINATED</td><td>127.0.0.1:10204</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">0.1   </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1217.83 </td><td style=\"text-align: right;\">   0.996933</td><td style=\"text-align: right;\">   0.9972</td></tr>\n",
       "<tr><td>train_FFNN_a4593dd2</td><td>TERMINATED</td><td>127.0.0.1:18212</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        128</td><td style=\"text-align: right;\">1e-05 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1309.08 </td><td style=\"text-align: right;\">   0.999978</td><td style=\"text-align: right;\">   0.999 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-23 08:18:21,970\tINFO tune.py:639 -- Total run time: 22056.30 seconds (22056.16 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_FFNN\r",
      " pid=18212)\u001b[0m Figure(1300x1300)\r\n",
      "\u001b[2m\u001b[36m(train_FFNN\r",
      " pid=18212)\u001b[0m Figure(1300x1300)\r\n"
     ]
    }
   ],
   "source": [
    "result_ffnn = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f952ae",
   "metadata": {},
   "source": [
    "BOHB Example: https://docs.ray.io/en/latest/tune/examples/includes/bohb_example.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
