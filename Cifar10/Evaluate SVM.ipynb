{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6bc7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'SVM'\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a184fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import HyperBandForBOHB\n",
    "from ray.tune.suggest.bohb import TuneBOHB\n",
    "import ConfigSpace as CS\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decb467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cpu')\n",
    "classes = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2eb792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(features, labels, model, dataset):\n",
    "    actual_classes = labels\n",
    "    predicted_classes = model.predict(features)\n",
    "    \n",
    "    #since labels are read in based on ordering in the folder, \n",
    "    #this corrects the labels so they reflect the correct classes\n",
    "    #############################################################\n",
    "    if dataset == 'gen':\n",
    "        labels_dict = {\n",
    "            0:3,\n",
    "            1:5,\n",
    "            2:7,\n",
    "            3:8\n",
    "        }\n",
    "        for index, item in enumerate(actual_classes):\n",
    "            actual_classes[index] = labels_dict[item]\n",
    "    #############################################################\n",
    "    \n",
    "    performance_report = classification_report(\n",
    "                        actual_classes, \n",
    "                        predicted_classes, \n",
    "                        labels=list(range(0,10)), \n",
    "                        target_names=classes, \n",
    "                        output_dict=True\n",
    "                        )\n",
    "\n",
    "    with open(f'performance_report_{dataset}.json', 'w') as f:\n",
    "        json.dump(performance_report, f, indent=0)\n",
    "\n",
    "    overall_accuracy = accuracy_score(actual_classes, predicted_classes)\n",
    "\n",
    "    comparison_list = [['Actual', 'Predicted']]\n",
    "\n",
    "    for i in range(0,len(actual_classes)):\n",
    "        comparison_list.append([actual_classes[i], predicted_classes[i]])\n",
    "    np.savetxt(f'class_pred_{dataset}.csv', comparison_list, delimiter=',', fmt='%s')\n",
    "    \n",
    "    labeled_actual = []\n",
    "    labeled_predicted = []\n",
    "    for index, item in enumerate(actual_classes):\n",
    "        labeled_actual.append(classes[actual_classes[index]])\n",
    "        labeled_predicted.append(classes[predicted_classes[index]])\n",
    "        \n",
    "    plt.clf()\n",
    "    c_matrix = confusion_matrix(labeled_actual, labeled_predicted)\n",
    "    c_df = pd.DataFrame(c_matrix, index=classes, columns=classes)\n",
    "    plt.figure(figsize=(13,13))\n",
    "    sns.heatmap(c_df, annot=True, fmt='g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.savefig(f'labeled_confusion_matrix_{dataset}.png', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    \n",
    "    return overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee85ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_kal\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\s_kal\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\s_kal\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 936x936 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 936x936 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtest = torch.load('C:/Users/s_kal/Desktop/9039-ML/Final Project/Code/test_extracted_features.pt',map_location=torch.device('cpu'))\n",
    "ytest = torch.load('C:/Users/s_kal/Desktop/9039-ML/Final Project/Code/test_extracted_labels.pt',map_location=torch.device('cpu'))\n",
    "xgen = torch.load('C:/Users/s_kal/Desktop/9039-ML/Final Project/Code/gen_extracted_features.pt',map_location=torch.device('cpu'))\n",
    "ygen = torch.load('C:/Users/s_kal/Desktop/9039-ML/Final Project/Code/gen_extracted_labels.pt',map_location=torch.device('cpu'))\n",
    "\n",
    "model = load('C:/Users/s_kal/Desktop/9039-ML/Final Project/Code/Evaluate Optimal Models/SVM Results/model_SVM.joblib')\n",
    "\n",
    "#     For learning curve of best model:\n",
    "#     plt.clf()\n",
    "#     plot_learning_curves(xtrain, ytrain, xtest, ytest, model)\n",
    "#     plt.savefig('learning_curve.png')\n",
    "\n",
    "test_acc =  model_eval(xtest, ytest, model, 'test')\n",
    "gen_acc = model_eval(xgen, ygen, model, 'gen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd429013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9761"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79138d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8719893798596624"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0a3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
