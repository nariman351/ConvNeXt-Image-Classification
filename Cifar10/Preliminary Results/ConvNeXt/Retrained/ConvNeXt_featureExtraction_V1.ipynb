{"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nNv4k9HEgJT6","executionInfo":{"status":"ok","timestamp":1658430651687,"user_tz":240,"elapsed":8627,"user":{"displayName":"Nariman H","userId":"16525869975551128991"}},"outputId":"4e149511-3617-4789-a4b3-1dc176de27f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 8.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 13.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 56.5 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 54.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-VAVxPNTxdHo"},"outputs":[],"source":["# Imports\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torchvision import models\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from numpy.lib.function_base import append\n","import numpy as np\n","from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61S23AXnftrG","executionInfo":{"status":"ok","timestamp":1658430655093,"user_tz":240,"elapsed":9,"user":{"displayName":"Nariman H","userId":"16525869975551128991"}},"outputId":"50bc4a05-eb41-4645-84c1-ba4818d9afcf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":3}],"source":["# Use CUDA/GPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240,"referenced_widgets":["cc07c20d293a481e8299e97b6c08b654","8fe31702e0304168badea38bf072525c","0708a92c7529466eb97f99e305cf998e","75abd27d43c54ae699e7f5bdaa700142","7981b97b81af4ab4a34b99d9956973a7","f2aa250c63b34087b35c539e64dcc98e","069dd6f283ea47a68f930c6a62d0e039","5e4d34a8269c448984f3b7253b94282e","ad704a98498f4b648ef0d8cde62baeda","a26ffe07e8914aadb29b0716a1dc7b05","bcc90f8400654e3baede063c1bf3cbd1"]},"id":"FFkaoDviftrH","executionInfo":{"status":"ok","timestamp":1658431026591,"user_tz":240,"elapsed":371504,"user":{"displayName":"Nariman H","userId":"16525869975551128991"}},"outputId":"3d840d4b-cae9-44d7-9ebc-f94ca999604d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc07c20d293a481e8299e97b6c08b654"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","[0.49147618 0.48220086 0.44667191] [0.24713163 0.24367339 0.26168631]\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n","---extracted train saved---\n","---extracted val saved---\n","---extracted test saved---\n","===Finished===\n"]}],"source":["#Seed Randomizers\n","random_seed = 42\n","np.random.seed(random_seed)\n","torch_rng = torch.manual_seed(random_seed)\n","\n","#Classes\n","classes = ('Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck')\n","\n","#Randomly Splitting Train set into Training and Validation\n","train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n","val_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n","\n","indices = list(range(len(train_data)))\n","np.random.shuffle(indices)\n","train_indices = indices[:45000]\n","val_indices = indices[45000:]\n","train_sample = SubsetRandomSampler(train_indices)\n","val_sample = SubsetRandomSampler(val_indices)\n","\n","# #Get mean and std from just the training set\n","train_sample_data = []\n","for index, item in enumerate(train_sample):\n","    train_sample_data.append(train_data[item])\n","\n","img_arr = np.concatenate([np.asarray(train_sample_data[i][0]) for i in range(len(train_sample_data))])\n","\n","train_mean = np.mean(img_arr, axis=(0, 1))/255\n","train_std = np.std(img_arr, axis=(0, 1))/255\n","print(train_mean, train_std)\n","\n","#Set Train and Test/Validation Image Transformers For Data Augmentation\n","train_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.RandomChoice(transforms=[\n","        transforms.RandomRotation(degrees=45), \n","        transforms.GaussianBlur(kernel_size=3),\n","        transforms.RandomHorizontalFlip(p=1),\n","        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),#from ConvNeXt\n","        transforms.RandomVerticalFlip(p=0)\n","    ], \n","        p=[0.05, 0.05, 0.05, 0.05, 0.8]),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=train_mean, std=train_std),\n","])\n","test_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=train_mean, std=train_std)\n","])\n","\n","#Reload/Transfor/DataLoader\n","train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n","val_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=test_transform)\n","\n","batch_size = 8\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sample, num_workers=2)\n","val_loader = torch.utils.data.DataLoader(val_data, batch_size=1, sampler=val_sample, num_workers=2)\n","\n","test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, num_workers=2)\n","\n","# gen_data = datasets.ImageFolder(root='./Generalization_Images', transform=test_transform)\n","# gen_loader = torch.utils.data.DataLoader(gen_data, batch_size=1, shuffle=False, num_workers=2)\n","\n","# Load Feature Extractor ConvNext\n","# feature_extractor = ConvNextFeatureExtractor.from_pretrained(\"facebook/convnext-tiny-224\")\n","# model = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-tiny-224\")\n","# We dont need the above code anymore\n","model = torch.load(\"Model_ConvNeXt_97\")\n","\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","model.eval()\n","\n","new_classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n","model.classifier = new_classifier\n","\n","model.to(device)\n","\n","# Saving Extracted Features - Tabular Modeling\n","def extract_save(data_loader, dataset, batch_size=1):\n","    x = []\n","    y = []\n","\n","    with torch.no_grad():\n","        for data in data_loader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            y.append(labels)\n","            x.append(model(images))\n","    \n","    features = []\n","    for index, item in enumerate(x):\n","        features.append(item.logits)\n","\n","    x = features\n","    x = torch.stack(x)\n","    y = torch.stack(y)\n","\n","    x_size = x.size()\n","    x = x.cpu()\n","    x = x.numpy()   \n","    x = x.T.reshape(x_size[2],x_size[0]*batch_size)\n","    x = x.T\n","\n","    y_size = y.size()\n","    y = y.cpu()\n","    y = y.numpy()\n","    y = y.T.reshape(1,y_size[0]*batch_size)\n","    y = y.flatten()\n","    \n","    torch.save(x, f'./{dataset}_extracted_features.pt')\n","    torch.save(y, f'./{dataset}_extracted_labels.pt')\n","    print(f'---extracted {dataset} saved---')\n","\n","extract_save(train_loader, 'train', batch_size=batch_size)\n","extract_save(val_loader, 'val')\n","extract_save(test_loader, 'test')\n","# extract_save(gen_loader, 'gen')\n","\n","# # Saving DataLoaders - CNN Modeling\n","# torch.save(train_loader, './train_cnn.pt')\n","# torch.save(val_loader, './val_cnn.pt')\n","# torch.save(test_loader, './test_cnn.pt')\n","# torch.save(gen_loader, './gen_cnn.pt')\n","\n","# print(f'---DataLoaders Saved---')\n","\n","# # Saving Mean and Std for Normalizing Images\n","# torch.save(train_mean, './normalizer_mean.pt')\n","# torch.save(train_std, './normalizer_std.pt')\n","\n","# print(f'---Normalizing Data Saved---')\n","\n","print('===Finished===')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yvow6C8aftrI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658365752859,"user_tz":240,"elapsed":4418,"user":{"displayName":"Nariman H","userId":"16525869975551128991"}},"outputId":"8a77b22b-0277-40fe-a6ce-3acdf7e28c12"},"outputs":[{"output_type":"stream","name":"stdout","text":["---DataLoaders Saved---\n","---Normalizing Data Saved---\n"]}],"source":["# # Saving DataLoaders - CNN Modeling\n","# torch.save(train_loader, './train_cnn.pt')\n","# torch.save(val_loader, './val_cnn.pt')\n","# torch.save(test_loader, './test_cnn.pt')\n","# # torch.save(gen_loader, './gen_cnn.pt')\n","\n","# print(f'---DataLoaders Saved---')\n","\n","# # Saving Mean and Std for Normalizing Images\n","# torch.save(train_mean, './normalizer_mean.pt')\n","# torch.save(train_std, './normalizer_std.pt')\n","\n","# print(f'---Normalizing Data Saved---')"]},{"cell_type":"code","source":["# # Getting xtest, ytest from the model\n","# xtest = []\n","# ytest = []\n","\n","# with torch.no_grad():\n","#     for data in test_loader:\n","#         images, labels = data[0].to(device), data[1].to(device)\n","#         ytest.append(labels)\n","#         xtest.append(model(images))\n"],"metadata":{"id":"D3R7iSFAVnYa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Getting xtrain, ytrain from the model\n","# xtrain = []\n","# ytrain = []\n","\n","# with torch.no_grad():\n","#     for data in train_loader:\n","#         images, labels = data[0].to(device), data[1].to(device)\n","#         ytrain.append(labels)\n","#         xtrain.append(model(images))"],"metadata":{"id":"s-fl0JDBVrxD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Saving xtest, ytest to use in another file\n","# torch.save(xtest, 'xtestconvnext.pt')\n","# torch.save(ytest, 'ytestconvnext.pt')"],"metadata":{"id":"iXLsUfPdVwU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Saving xtrain, ytrain to use in another file\n","# torch.save(xtrain, 'xtrainconvnext.pt')\n","# torch.save(ytrain, 'ytrainconvnext.pt')"],"metadata":{"id":"9XQl8QGAVxwd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ConvNeXt_featureExtraction_V1.ipynb","provenance":[{"file_id":"1V_SsZRzpwrhUTAhn7tXuKwhbnKoR15aE","timestamp":1648933339354},{"file_id":"1JMydR7Cs3IZaG3rkmVtBRiB7c3w9djZy","timestamp":1648931443727}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cc07c20d293a481e8299e97b6c08b654":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8fe31702e0304168badea38bf072525c","IPY_MODEL_0708a92c7529466eb97f99e305cf998e","IPY_MODEL_75abd27d43c54ae699e7f5bdaa700142"],"layout":"IPY_MODEL_7981b97b81af4ab4a34b99d9956973a7"}},"8fe31702e0304168badea38bf072525c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2aa250c63b34087b35c539e64dcc98e","placeholder":"​","style":"IPY_MODEL_069dd6f283ea47a68f930c6a62d0e039","value":"100%"}},"0708a92c7529466eb97f99e305cf998e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e4d34a8269c448984f3b7253b94282e","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad704a98498f4b648ef0d8cde62baeda","value":170498071}},"75abd27d43c54ae699e7f5bdaa700142":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a26ffe07e8914aadb29b0716a1dc7b05","placeholder":"​","style":"IPY_MODEL_bcc90f8400654e3baede063c1bf3cbd1","value":" 170498071/170498071 [00:03&lt;00:00, 49613777.22it/s]"}},"7981b97b81af4ab4a34b99d9956973a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2aa250c63b34087b35c539e64dcc98e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"069dd6f283ea47a68f930c6a62d0e039":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e4d34a8269c448984f3b7253b94282e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad704a98498f4b648ef0d8cde62baeda":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a26ffe07e8914aadb29b0716a1dc7b05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcc90f8400654e3baede063c1bf3cbd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}